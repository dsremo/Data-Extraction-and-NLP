{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b75eff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article for blackassign0001 saved successfully.\n",
      "Article for blackassign0002 saved successfully.\n",
      "Article for blackassign0003 saved successfully.\n",
      "Article for blackassign0004 saved successfully.\n",
      "Article for blackassign0005 saved successfully.\n",
      "Article for blackassign0006 saved successfully.\n",
      "Article for blackassign0007 saved successfully.\n",
      "Article for blackassign0008 saved successfully.\n",
      "Article for blackassign0009 saved successfully.\n",
      "Article for blackassign0010 saved successfully.\n",
      "Article for blackassign0011 saved successfully.\n",
      "Article for blackassign0012 saved successfully.\n",
      "Article for blackassign0013 saved successfully.\n",
      "Article for blackassign0014 saved successfully.\n",
      "Article for blackassign0015 saved successfully.\n",
      "Article for blackassign0016 saved successfully.\n",
      "Article for blackassign0017 saved successfully.\n",
      "Article for blackassign0018 saved successfully.\n",
      "Article for blackassign0019 saved successfully.\n",
      "Article for blackassign0020 saved successfully.\n",
      "Article for blackassign0021 saved successfully.\n",
      "Article for blackassign0022 saved successfully.\n",
      "Article for blackassign0023 saved successfully.\n",
      "Article for blackassign0024 saved successfully.\n",
      "Article for blackassign0025 saved successfully.\n",
      "Article for blackassign0026 saved successfully.\n",
      "Article for blackassign0027 saved successfully.\n",
      "Article for blackassign0028 saved successfully.\n",
      "Article for blackassign0029 saved successfully.\n",
      "Article for blackassign0030 saved successfully.\n",
      "Article for blackassign0031 saved successfully.\n",
      "Article for blackassign0032 saved successfully.\n",
      "Article for blackassign0033 saved successfully.\n",
      "Article for blackassign0034 saved successfully.\n",
      "Article for blackassign0035 saved successfully.\n",
      "Failed to retrieve the webpage. Status code: 404\n",
      "Failed to extract article for blackassign0036.\n",
      "Article for blackassign0037 saved successfully.\n",
      "Article for blackassign0038 saved successfully.\n",
      "Article for blackassign0039 saved successfully.\n",
      "Article for blackassign0040 saved successfully.\n",
      "Article for blackassign0041 saved successfully.\n",
      "Article for blackassign0042 saved successfully.\n",
      "Article for blackassign0043 saved successfully.\n",
      "Article for blackassign0044 saved successfully.\n",
      "Article for blackassign0045 saved successfully.\n",
      "Article for blackassign0046 saved successfully.\n",
      "Article for blackassign0047 saved successfully.\n",
      "Article for blackassign0048 saved successfully.\n",
      "Failed to retrieve the webpage. Status code: 404\n",
      "Failed to extract article for blackassign0049.\n",
      "Article for blackassign0050 saved successfully.\n",
      "Article for blackassign0051 saved successfully.\n",
      "Article for blackassign0052 saved successfully.\n",
      "Article for blackassign0053 saved successfully.\n",
      "Article for blackassign0054 saved successfully.\n",
      "Article for blackassign0055 saved successfully.\n",
      "Article for blackassign0056 saved successfully.\n",
      "Article for blackassign0057 saved successfully.\n",
      "Article for blackassign0058 saved successfully.\n",
      "Article for blackassign0059 saved successfully.\n",
      "Article for blackassign0060 saved successfully.\n",
      "Article for blackassign0061 saved successfully.\n",
      "Article for blackassign0062 saved successfully.\n",
      "Article for blackassign0063 saved successfully.\n",
      "Article for blackassign0064 saved successfully.\n",
      "Article for blackassign0065 saved successfully.\n",
      "Article for blackassign0066 saved successfully.\n",
      "Article for blackassign0067 saved successfully.\n",
      "Article for blackassign0068 saved successfully.\n",
      "Article for blackassign0069 saved successfully.\n",
      "Article for blackassign0070 saved successfully.\n",
      "Article for blackassign0071 saved successfully.\n",
      "Article for blackassign0072 saved successfully.\n",
      "Article for blackassign0073 saved successfully.\n",
      "Article for blackassign0074 saved successfully.\n",
      "Article for blackassign0075 saved successfully.\n",
      "Article for blackassign0076 saved successfully.\n",
      "Article for blackassign0077 saved successfully.\n",
      "Article for blackassign0078 saved successfully.\n",
      "Article for blackassign0079 saved successfully.\n",
      "Article for blackassign0080 saved successfully.\n",
      "Article for blackassign0081 saved successfully.\n",
      "Article for blackassign0082 saved successfully.\n",
      "Article for blackassign0083 saved successfully.\n",
      "Article for blackassign0084 saved successfully.\n",
      "Article for blackassign0085 saved successfully.\n",
      "Article for blackassign0086 saved successfully.\n",
      "Article for blackassign0087 saved successfully.\n",
      "Article for blackassign0088 saved successfully.\n",
      "Article for blackassign0089 saved successfully.\n",
      "Article for blackassign0090 saved successfully.\n",
      "Article for blackassign0091 saved successfully.\n",
      "Article for blackassign0092 saved successfully.\n",
      "Article for blackassign0093 saved successfully.\n",
      "Article for blackassign0094 saved successfully.\n",
      "Article for blackassign0095 saved successfully.\n",
      "Article for blackassign0096 saved successfully.\n",
      "Article for blackassign0097 saved successfully.\n",
      "Article for blackassign0098 saved successfully.\n",
      "Article for blackassign0099 saved successfully.\n",
      "Article for blackassign0100 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "titles = []\n",
    "articles = []\n",
    "\n",
    "def scrape_title_and_content(url):\n",
    "  try:\n",
    "      # Send a GET request to the URL\n",
    "      response = requests.get(url)\n",
    "\n",
    "      # Check if the request was successful (status code 200)\n",
    "      if response.status_code == 200:\n",
    "          # Parse the HTML content of the page\n",
    "          soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "          # Find the title\n",
    "          title = soup.find('h1', class_='entry-title')\n",
    "          if title:\n",
    "              #print(f'Title: {title.text.strip()}')\n",
    "              titles.append(title.text.strip())\n",
    "          else:\n",
    "            title = soup.find('h1', class_='tdb-title-text')\n",
    "            #print(f'Title: {title.text.strip()}')\n",
    "            titles.append(title.text.strip())\n",
    "            \n",
    "\n",
    "\n",
    "          # Find the content\n",
    "          content = soup.find('div', class_='td-post-content tagdiv-type')  # Adjust the class as needed\n",
    "          if content:\n",
    "              #print('Content:')\n",
    "              #print(content.text.strip())\n",
    "              articles.append(content.text.strip())\n",
    "          else:\n",
    "            content = soup.find('div', class_='td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')  # Adjust the class as needed\n",
    "            #print('Content:')\n",
    "            #print(content.text.strip())\n",
    "            articles.append(content.text.strip())\n",
    "          return title.text.strip() if title else None, content.text.strip() if content else None\n",
    "            \n",
    "      else:\n",
    "          print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "  except Exception as e:\n",
    "        print(f\"An error occurred while scraping URL: {url}. Error: {str(e)}\")\n",
    "  return None, None\n",
    "# Read input Excel file\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Loop through each row\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Check if the URL is valid\n",
    "    if not pd.isna(url):\n",
    "        # Scrape title and content\n",
    "        title, content = scrape_title_and_content(url)\n",
    "\n",
    "        if title and content:\n",
    "            # Save the article to a text file\n",
    "            filename = f'{url_id}.txt'\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(f'Title: {title}\\n\\n')\n",
    "                file.write(f'Content: {content}\\n\\n')\n",
    "            print(f'Article for {url_id} saved successfully.')\n",
    "\n",
    "        else:\n",
    "            print(f'Failed to extract article for {url_id}.')\n",
    "    else:\n",
    "        print(f'Invalid URL for {url_id}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a486e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk in c:\\users\\ds_remo\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     - ------------------------------------- 30.7/636.8 kB 1.3 MB/s eta 0:00:01\n",
      "     - ------------------------------------- 30.7/636.8 kB 1.3 MB/s eta 0:00:01\n",
      "     ---- -------------------------------- 71.7/636.8 kB 491.5 kB/s eta 0:00:02\n",
      "     ------ ----------------------------- 112.6/636.8 kB 595.3 kB/s eta 0:00:01\n",
      "     ------ ----------------------------- 122.9/636.8 kB 554.9 kB/s eta 0:00:01\n",
      "     ----------- ------------------------ 204.8/636.8 kB 731.4 kB/s eta 0:00:01\n",
      "     ------------ ----------------------- 225.3/636.8 kB 765.3 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 256.0/636.8 kB 684.6 kB/s eta 0:00:01\n",
      "     --------------- -------------------- 276.5/636.8 kB 710.0 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 337.9/636.8 kB 776.5 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 399.4/636.8 kB 778.2 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 491.5/636.8 kB 879.9 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 512.0/636.8 kB 892.8 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 532.5/636.8 kB 814.7 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 532.5/636.8 kB 814.7 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 532.5/636.8 kB 814.7 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 593.9/636.8 kB 747.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 636.8/636.8 kB 771.6 kB/s eta 0:00:00\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "     ---------------------------------------- 0.0/105.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 105.1/105.1 kB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\ds_remo\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ds_remo\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ds_remo\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ds_remo\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.0 MB 1.7 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/2.0 MB 1.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/2.0 MB 1.4 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.2/2.0 MB 1.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.0 MB 983.0 kB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.0 MB 983.9 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.4/2.0 MB 1.0 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.4/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.5/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.5/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.6/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.6/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.6/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.7/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 0.8/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/2.0 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 0.9/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.0/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.1/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.2/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.2/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.2/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.3/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.3/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.4/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.4/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.5/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.6/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.7/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.7/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.8/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.9/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.9/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.9/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ds_remo\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py): started\n",
      "  Building wheel for docx2txt (setup.py): finished with status 'done'\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3973 sha256=0a951c78ca8fe7ac7b07f4b38f2f6a561877e332db85544a260d812ca8535625\n",
      "  Stored in directory: c:\\users\\ds_remo\\appdata\\local\\pip\\cache\\wheels\\40\\75\\01\\e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt, pyphen, textstat, textblob\n",
      "Successfully installed docx2txt-0.8 pyphen-0.14.0 textblob-0.17.1 textstat-0.7.3\n"
     ]
    }
   ],
   "source": [
    "! pip install docx2txt nltk textblob textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76415e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import textstat\n",
    "\n",
    "# Download NLTK stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to load stopwords from multiple text files\n",
    "def load_stopwords(stopwords_files):\n",
    "    custom_stop_words = []\n",
    "    for file_path in stopwords_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            custom_stop_words.extend(file.read().splitlines())\n",
    "    return custom_stop_words\n",
    "\n",
    "# Function to clean text using custom stop words\n",
    "def clean_text(text, custom_stop_words):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in custom_stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to load positive and negative words from text files\n",
    "def load_pos_neg_words(pos_file, neg_file):\n",
    "    with open(pos_file, 'r', encoding='utf-8') as file:\n",
    "        positive_words = file.read().splitlines()\n",
    "    with open(neg_file, 'r', encoding='utf-8') as file:\n",
    "        negative_words = file.read().splitlines()\n",
    "    return positive_words, negative_words\n",
    "\n",
    "# Function to create a dictionary of positive and negative words\n",
    "def create_pos_neg_dict(positive_words, negative_words):\n",
    "    pos_neg_dict = {}\n",
    "    for word in positive_words:\n",
    "        pos_neg_dict[word.lower()] = 'positive'\n",
    "    for word in negative_words:\n",
    "        pos_neg_dict[word.lower()] = 'negative'\n",
    "    return pos_neg_dict\n",
    "\n",
    "# Function to extract derived variables\n",
    "def extract_derived_variables(text):\n",
    "    # Create a TextBlob object for the text\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Compute sentiment scores\n",
    "    sentiment_scores = blob.sentiment\n",
    "\n",
    "    # Analysis of Readability\n",
    "    readability_score = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    # Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = textstat.lexicon_count(text) / textstat.sentence_count(text)\n",
    "    \n",
    "    # Complex Word Count\n",
    "    complex_word_count = textstat.difficult_words(text)\n",
    "    \n",
    "    # Word Count\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    \n",
    "    # Syllable Count Per Word\n",
    "    syllables_per_word = textstat.syllable_count(text) / textstat.lexicon_count(text)\n",
    "    \n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "    \n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    derived_variables = {\n",
    "        'Sentiment Scores': sentiment_scores,\n",
    "        'Readability Score': readability_score,\n",
    "        'Average Number of Words Per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': word_count,\n",
    "        'Syllable Count Per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Average Word Length': avg_word_length\n",
    "    }\n",
    "\n",
    "    return derived_variables\n",
    "\n",
    "# Specify paths to your files\n",
    "stopwords_files = [\n",
    "   'StopWords_Names.txt',\n",
    "    'StopWords_Geographic.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_Auditor.txt',\n",
    "    'StopWords_Currencies.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_DatesandNumbers.txt',\n",
    "]\n",
    "\n",
    "positive_words_file = 'positive-words.txt'\n",
    "negative_words_file = 'negative-words.txt'\n",
    "input_excel_file = 'Input.xlsx'\n",
    "\n",
    "# Load stopwords, positive words, and negative words\n",
    "custom_stop_words = load_stopwords(stopwords_files)\n",
    "positive_words, negative_words = load_pos_neg_words(positive_words_file, negative_words_file)\n",
    "\n",
    "# Read input Excel file\n",
    "df = pd.read_excel(input_excel_file)\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    text_content = row['Text_Column_Name']  # Replace 'Text_Column_Name' with the actual name of the column containing text\n",
    "\n",
    "    # Clean text using custom stop words\n",
    "    cleaned_text = clean_text(text_content, custom_stop_words)\n",
    "\n",
    "    # Create a dictionary of positive and negative words\n",
    "    pos_neg_dict = create_pos_neg_dict(positive_words, negative_words)\n",
    "\n",
    "    # Analyze text and extract derived variables\n",
    "    derived_variables = extract_derived_variables(cleaned_text)\n",
    "\n",
    "    # Print or use the derived variables as needed\n",
    "    print(f\"Derived Variables for Row {index + 1}: {derived_variables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import textstat\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to load stopwords from a text file\n",
    "def load_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "# Function to clean text using custom stop words\n",
    "def clean_text(text, custom_stop_words):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in custom_stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to create a dictionary of positive and negative words\n",
    "def create_pos_neg_dict(positive_words, negative_words):\n",
    "    pos_neg_dict = {}\n",
    "    for word in positive_words:\n",
    "        pos_neg_dict[word.lower()] = 'positive'\n",
    "    for word in negative_words:\n",
    "        pos_neg_dict[word.lower()] = 'negative'\n",
    "    return pos_neg_dict\n",
    "\n",
    "# Function to extract derived variables\n",
    "def extract_derived_variables(text):\n",
    "    # Create a TextBlob object for the text\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Compute sentiment scores\n",
    "    sentiment_scores = blob.sentiment\n",
    "\n",
    "    # Analysis of Readability\n",
    "    readability_score = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    # Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = textstat.lexicon_count(text) / textstat.sentence_count(text)\n",
    "    \n",
    "    # Complex Word Count\n",
    "    complex_word_count = textstat.difficult_words(text)\n",
    "    \n",
    "    # Word Count\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    \n",
    "    # Syllable Count Per Word\n",
    "    syllables_per_word = textstat.syllable_count(text) / textstat.lexicon_count(text)\n",
    "    \n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "    \n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    derived_variables = {\n",
    "        'Sentiment Scores': sentiment_scores,\n",
    "        'Readability Score': readability_score,\n",
    "        'Average Number of Words Per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': word_count,\n",
    "        'Syllable Count Per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Average Word Length': avg_word_length\n",
    "    }\n",
    "\n",
    "    return derived_variables\n",
    "\n",
    "# Specify paths to your files\n",
    "stopwords_file = 'path/to/your/stopwords.txt'\n",
    "positive_words_file = 'path/to/your/positive_words.txt'\n",
    "negative_words_file = 'path/to/your/negative_words.txt'\n",
    "output_excel_file = 'path/to/your/output.xlsx'\n",
    "\n",
    "# Load stopwords, positive words, and negative words\n",
    "custom_stop_words = load_stopwords(stopwords_file)\n",
    "positive_words, negative_words = load_pos_neg_words(positive_words_file, negative_words_file)\n",
    "\n",
    "# Create a dictionary of positive and negative words\n",
    "pos_neg_dict = create_pos_neg_dict(positive_words, negative_words)\n",
    "\n",
    "# Create a DataFrame to store the derived variables\n",
    "output_df = pd.DataFrame(columns=[\n",
    "    'URL_ID',\n",
    "    'URL',\n",
    "    'POSITIVE SCORE',\n",
    "    'NEGATIVE SCORE',\n",
    "    'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH',\n",
    "    'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT',\n",
    "    'SYLLABLE PER WORD',\n",
    "    'PERSONAL PRONOUNS',\n",
    "    'AVG WORD LENGTH'\n",
    "])\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    filename = f'{url_id}.txt'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        # Read the content from the file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # Clean text using custom stop words\n",
    "        cleaned_text = clean_text(text_content, custom_stop_words)\n",
    "\n",
    "        # Analyze text and extract derived variables\n",
    "        derived_variables = extract_derived_variables(cleaned_text)\n",
    "\n",
    "        # Append the results to the output DataFrame\n",
    "        output_df = output_df.append({\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'POSITIVE SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'NEGATIVE SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'POLARITY SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'SUBJECTIVITY SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'AVG SENTENCE LENGTH': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': (derived_variables['Complex Word Count'] / derived_variables['Word Count']) * 100,\n",
    "            'FOG INDEX': derived_variables['FOG INDEX'],  # You need to implement this in the extract_derived_variables function\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'COMPLEX WORD COUNT': derived_variables['Complex Word Count'],\n",
    "            'WORD COUNT': derived_variables['Word Count'],\n",
    "            'SYLLABLE PER WORD': derived_variables['Syllable Count Per Word'],\n",
    "            'PERSONAL PRONOUNS': derived_variables['Personal Pronouns'],\n",
    "            'AVG WORD LENGTH': derived_variables['Average Word Length']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        print(f'Textual analysis for {url_id} completed successfully.')\n",
    "    else:\n",
    "        print(f'File {filename} not found for {url_id}.')\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_df.to_excel(output_excel_file, index=False)\n",
    "print(f'Results saved to {output_excel_file}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcfff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd3 in position 184: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m input_excel_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Load stopwords, positive words, and negative words\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m custom_stop_words \u001b[38;5;241m=\u001b[39m \u001b[43mload_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopwords_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m positive_words, negative_words \u001b[38;5;241m=\u001b[39m load_pos_neg_words(positive_words_file, negative_words_file)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Create a dictionary of positive and negative words\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mload_stopwords\u001b[1;34m(stopwords_files)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m stopwords_files:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 15\u001b[0m         custom_stop_words\u001b[38;5;241m.\u001b[39mextend(\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplitlines())\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m custom_stop_words\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd3 in position 184: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import textstat\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to load stopwords from multiple text files\n",
    "def load_stopwords(stopwords_files):\n",
    "    custom_stop_words = []\n",
    "    for file_path in stopwords_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            custom_stop_words.extend(file.read().splitlines())\n",
    "    return custom_stop_words\n",
    "# Function to clean text using custom stop words\n",
    "def clean_text(text, custom_stop_words):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in custom_stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "    \n",
    "# Function to load positive and negative words from text files\n",
    "def load_pos_neg_words(pos_file, neg_file):\n",
    "    with open(pos_file, 'r', encoding='utf-8') as file:\n",
    "        positive_words = file.read().splitlines()\n",
    "    with open(neg_file, 'r', encoding='utf-8') as file:\n",
    "        negative_words = file.read().splitlines()\n",
    "    return positive_words, negative_words\n",
    "# Function to create a dictionary of positive and negative words\n",
    "def create_pos_neg_dict(positive_words, negative_words):\n",
    "    pos_neg_dict = {}\n",
    "    for word in positive_words:\n",
    "        pos_neg_dict[word.lower()] = 'positive'\n",
    "    for word in negative_words:\n",
    "        pos_neg_dict[word.lower()] = 'negative'\n",
    "    return pos_neg_dict\n",
    "\n",
    "# Function to extract derived variables\n",
    "def extract_derived_variables(text):\n",
    "    # Create a TextBlob object for the text\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Compute sentiment scores\n",
    "    sentiment_scores = blob.sentiment\n",
    "\n",
    "    # Analysis of Readability\n",
    "    readability_score = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    # Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = textstat.lexicon_count(text) / textstat.sentence_count(text)\n",
    "    \n",
    "    # Complex Word Count\n",
    "    complex_word_count = textstat.difficult_words(text)\n",
    "    \n",
    "    # Word Count\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    \n",
    "    # Syllable Count Per Word\n",
    "    syllables_per_word = textstat.syllable_count(text) / textstat.lexicon_count(text)\n",
    "    \n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "    \n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    derived_variables = {\n",
    "        'Sentiment Scores': sentiment_scores,\n",
    "        'Readability Score': readability_score,\n",
    "        'Average Number of Words Per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': word_count,\n",
    "        'Syllable Count Per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Average Word Length': avg_word_length\n",
    "    }\n",
    "\n",
    "    return derived_variables\n",
    "stopwords_files = [\n",
    "   'StopWords_Names.txt',\n",
    "    'StopWords_Geographic.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_Auditor.txt',\n",
    "    'StopWords_Currencies.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_DatesandNumbers.txt',\n",
    "]\n",
    "\n",
    "positive_words_file = 'positive-words.txt'\n",
    "negative_words_file = 'negative-words.txt'\n",
    "input_excel_file = 'Input.xlsx'\n",
    "\n",
    "# Load stopwords, positive words, and negative words\n",
    "custom_stop_words = load_stopwords(stopwords_files)\n",
    "positive_words, negative_words = load_pos_neg_words(positive_words_file, negative_words_file)\n",
    "\n",
    "# Create a dictionary of positive and negative words\n",
    "pos_neg_dict = create_pos_neg_dict(positive_words, negative_words)\n",
    "\n",
    "# Create a DataFrame to store the derived variables\n",
    "output_df = pd.DataFrame(columns=[\n",
    "    'URL_ID',\n",
    "    'URL',\n",
    "    'POSITIVE SCORE',\n",
    "    'NEGATIVE SCORE',\n",
    "    'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH',\n",
    "    'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT',\n",
    "    'SYLLABLE PER WORD',\n",
    "    'PERSONAL PRONOUNS',\n",
    "    'AVG WORD LENGTH'\n",
    "])\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    filename = f'{url_id}.txt'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        # Read the content from the file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # Clean text using custom stop words\n",
    "        cleaned_text = clean_text(text_content, custom_stop_words)\n",
    "\n",
    "        # Analyze text and extract derived variables\n",
    "        derived_variables = extract_derived_variables(cleaned_text)\n",
    "\n",
    "        # Append the results to the output DataFrame\n",
    "        output_df = output_df.append({\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'POSITIVE SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'NEGATIVE SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'POLARITY SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'SUBJECTIVITY SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'AVG SENTENCE LENGTH': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': (derived_variables['Complex Word Count'] / derived_variables['Word Count']) * 100,\n",
    "            'FOG INDEX': derived_variables['FOG INDEX'],  # You need to implement this in the extract_derived_variables function\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'COMPLEX WORD COUNT': derived_variables['Complex Word Count'],\n",
    "            'WORD COUNT': derived_variables['Word Count'],\n",
    "            'SYLLABLE PER WORD': derived_variables['Syllable Count Per Word'],\n",
    "            'PERSONAL PRONOUNS': derived_variables['Personal Pronouns'],\n",
    "            'AVG WORD LENGTH': derived_variables['Average Word Length']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        print(f'Textual analysis for {url_id} completed successfully.')\n",
    "    else:\n",
    "        print(f'File {filename} not found for {url_id}.')\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_df.to_excel(output_excel_file, index=False)\n",
    "print(f'Results saved to {output_excel_file}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b3e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f033d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8db8dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10061] No\n",
      "[nltk_data]     connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import textstat\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stopwords from multiple text files\n",
    "def load_stopwords(stopwords_files, encoding='utf-8'):\n",
    "    custom_stop_words = []\n",
    "    for file_path in stopwords_files:\n",
    "        with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "            custom_stop_words.extend(file.read().splitlines())\n",
    "    return custom_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b291009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text using custom stop words\n",
    "def clean_text(text, custom_stop_words):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in custom_stop_words]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load positive and negative words from text files\n",
    "def load_pos_neg_words(pos_file, neg_file, encoding='utf-8'):\n",
    "    with open(pos_file, 'r', encoding=encoding, errors='ignore') as file:\n",
    "        positive_words = file.read().splitlines()\n",
    "    with open(neg_file, 'r', encoding=encoding, errors='ignore') as file:\n",
    "        negative_words = file.read().splitlines()\n",
    "    return positive_words, negative_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dictionary of positive and negative words\n",
    "def create_pos_neg_dict(positive_words, negative_words):\n",
    "    pos_neg_dict = {}\n",
    "    for word in positive_words:\n",
    "        pos_neg_dict[word.lower()] = 'positive'\n",
    "    for word in negative_words:\n",
    "        pos_neg_dict[word.lower()] = 'negative'\n",
    "    return pos_neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a193e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract derived variables\n",
    "def extract_derived_variables(text):\n",
    "    # Create a TextBlob object for the text\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Compute sentiment scores\n",
    "    sentiment_scores = blob.sentiment\n",
    "\n",
    "    # Analysis of Readability\n",
    "    readability_score = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    # Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = textstat.lexicon_count(text) / textstat.sentence_count(text)\n",
    "\n",
    "    # Complex Word Count\n",
    "    complex_word_count = textstat.difficult_words(text)\n",
    "\n",
    "    # Word Count\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "\n",
    "    # Syllable Count Per Word\n",
    "    syllables_per_word = textstat.syllable_count(text) / textstat.lexicon_count(text)\n",
    "\n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "\n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    derived_variables = {\n",
    "        'Sentiment Scores': sentiment_scores,\n",
    "        'Readability Score': readability_score,\n",
    "        'Average Number of Words Per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': word_count,\n",
    "        'Syllable Count Per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Average Word Length': avg_word_length\n",
    "    }\n",
    "\n",
    "    return derived_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file paths\n",
    "stopwords_files = [\n",
    "    'StopWords_Names.txt',\n",
    "    'StopWords_Geographic.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_Auditor.txt',\n",
    "    'StopWords_Currencies.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_DatesandNumbers.txt',\n",
    "]\n",
    "\n",
    "positive_words_file = 'positive-words.txt'\n",
    "negative_words_file = 'negative-words.txt'\n",
    "input_excel_file = 'Input.xlsx'\n",
    "output_excel_file = 'Output.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords, positive words, and negative words\n",
    "custom_stop_words = load_stopwords(stopwords_files, encoding='latin-1')\n",
    "positive_words, negative_words = load_pos_neg_words(positive_words_file, negative_words_file, encoding='latin-1')\n",
    "\n",
    "# Create a dictionary of positive and negative words\n",
    "pos_neg_dict = create_pos_neg_dict(positive_words, negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the derived variables\n",
    "output_df = pd.DataFrame(columns=[\n",
    "    'URL_ID',\n",
    "    'URL',\n",
    "    'POSITIVE SCORE',\n",
    "    'NEGATIVE SCORE',\n",
    "    'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH',\n",
    "    'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT',\n",
    "    'SYLLABLE PER WORD',\n",
    "    'PERSONAL PRONOUNS',\n",
    "    'AVG WORD LENGTH'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29380b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input Excel file\n",
    "df = pd.read_excel(input_excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    filename = f'{url_id}.txt'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        # Read the content from the file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # Clean text using custom stop words\n",
    "        cleaned_text = clean_text(text_content, custom_stop_words)\n",
    "\n",
    "        # Analyze text and extract derived variables\n",
    "        derived_variables = extract_derived_variables(cleaned_text)\n",
    "\n",
    "        # Append the results to the output DataFrame\n",
    "        output_df = output_df.append({\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'POSITIVE SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'NEGATIVE SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'POLARITY SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'SUBJECTIVITY SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'AVG SENTENCE LENGTH': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': (derived_variables['Complex Word Count'] / derived_variables['Word Count']) * 100,\n",
    "            'FOG INDEX': 0,  # You need to implement this in the extract_derived_variables function\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'COMPLEX WORD COUNT': derived_variables['Complex Word Count'],\n",
    "            'WORD COUNT': derived_variables['Word Count'],\n",
    "            'SYLLABLE PER WORD': derived_variables['Syllable Count Per Word'],\n",
    "            'PERSONAL PRONOUNS': derived_variables['Personal Pronouns'],\n",
    "            'AVG WORD LENGTH': derived_variables['Average Word Length']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        print(f'Textual analysis for {url_id} completed successfully.')\n",
    "    else:\n",
    "        print(f'File {filename} not found for {url_id}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85155ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an Excel file\n",
    "output_df.to_excel(output_excel_file, index=False)\n",
    "print(f'Results saved to {output_excel_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b2e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ds_remo/nltk_data'\n",
      "    - 'C:\\\\Users\\\\ds_remo\\\\anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ds_remo\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ds_remo\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ds_remo\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\tokenizers.py:57\u001b[0m, in \u001b[0;36mSentenceTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Return a list of sentences.'''\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ds_remo/nltk_data'\n    - 'C:\\\\Users\\\\ds_remo\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ds_remo\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ds_remo\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ds_remo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 143\u001b[0m\n\u001b[0;32m    140\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m clean_text(text_content, custom_stop_words)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Analyze text and extract derived variables\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m derived_variables \u001b[38;5;241m=\u001b[39m \u001b[43mextract_derived_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Append the results to the output DataFrame\u001b[39;00m\n\u001b[0;32m    146\u001b[0m output_df \u001b[38;5;241m=\u001b[39m output_df\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m: url_id,\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m: url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAVG WORD LENGTH\u001b[39m\u001b[38;5;124m'\u001b[39m: derived_variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Word Length\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    162\u001b[0m }, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[12], line 65\u001b[0m, in \u001b[0;36mextract_derived_variables\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     62\u001b[0m syllables_per_word \u001b[38;5;241m=\u001b[39m textstat\u001b[38;5;241m.\u001b[39msyllable_count(text) \u001b[38;5;241m/\u001b[39m textstat\u001b[38;5;241m.\u001b[39mlexicon_count(text)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Personal Pronouns\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m personal_pronouns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyself\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Average Word Length\u001b[39;00m\n\u001b[0;32m     68\u001b[0m avg_word_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mwords) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(blob\u001b[38;5;241m.\u001b[39mwords)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 24\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\blob.py:678\u001b[0m, in \u001b[0;36mTextBlob.words\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;124;03m    If you want to include punctuation characters, access the ``tokens``\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03m    property.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m    :returns: A :class:`WordList <WordList>` of word tokens.\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WordList(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_punc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\tokenizers.py:73\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, include_punc, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, include_punc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function for tokenizing text into words.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    tokenized to sentences before being tokenized to words.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     words \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m     71\u001b[0m         _word_tokenizer\u001b[38;5;241m.\u001b[39mitokenize(sentence, include_punc\u001b[38;5;241m=\u001b[39minclude_punc,\n\u001b[0;32m     72\u001b[0m                                 \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 73\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\base.py:64\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[1;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    :rtype: generator\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import textstat\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to load stopwords from multiple text files\n",
    "def load_stopwords(stopwords_files, encoding='utf-8'):\n",
    "    custom_stop_words = []\n",
    "    for file_path in stopwords_files:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            custom_stop_words.extend(file.read().splitlines())\n",
    "    return custom_stop_words\n",
    "\n",
    "# Function to clean text using custom stop words\n",
    "def clean_text(text, custom_stop_words):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in custom_stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to load positive and negative words from text files\n",
    "def load_pos_neg_words(pos_file, neg_file, encoding='utf-8'):\n",
    "    with open(pos_file, 'r', encoding=encoding) as file:\n",
    "        positive_words = file.read().splitlines()\n",
    "    with open(neg_file, 'r', encoding=encoding) as file:\n",
    "        negative_words = file.read().splitlines()\n",
    "    return positive_words, negative_words\n",
    "\n",
    "# Function to create a dictionary of positive and negative words\n",
    "def create_pos_neg_dict(positive_words, negative_words):\n",
    "    pos_neg_dict = {}\n",
    "    for word in positive_words:\n",
    "        pos_neg_dict[word.lower()] = 'positive'\n",
    "    for word in negative_words:\n",
    "        pos_neg_dict[word.lower()] = 'negative'\n",
    "    return pos_neg_dict\n",
    "\n",
    "# Function to extract derived variables\n",
    "def extract_derived_variables(text):\n",
    "    # Create a TextBlob object for the text\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Compute sentiment scores\n",
    "    sentiment_scores = blob.sentiment\n",
    "\n",
    "    # Analysis of Readability\n",
    "    readability_score = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    # Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = textstat.lexicon_count(text) / textstat.sentence_count(text)\n",
    "\n",
    "    # Complex Word Count\n",
    "    complex_word_count = textstat.difficult_words(text)\n",
    "\n",
    "    # Word Count\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "\n",
    "    # Syllable Count Per Word\n",
    "    syllables_per_word = textstat.syllable_count(text) / textstat.lexicon_count(text)\n",
    "\n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = sum(1 for word in blob.words if word.lower() in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "\n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    derived_variables = {\n",
    "        'Sentiment Scores': sentiment_scores,\n",
    "        'Readability Score': readability_score,\n",
    "        'Average Number of Words Per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': word_count,\n",
    "        'Syllable Count Per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Average Word Length': avg_word_length\n",
    "    }\n",
    "\n",
    "    return derived_variables\n",
    "\n",
    "# Specify the path to stopwords files, positive and negative words files\n",
    "stopwords_files = [\n",
    "   'StopWords_Names.txt',\n",
    "    'StopWords_Geographic.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_Auditor.txt',\n",
    "    'StopWords_Currencies.txt',\n",
    "    'StopWords_GenericLong.txt',\n",
    "    'StopWords_DatesandNumbers.txt',\n",
    "]\n",
    "\n",
    "positive_words_file = 'positive-words.txt'\n",
    "negative_words_file = 'negative-words.txt'\n",
    "input_excel_file = 'Input.xlsx'\n",
    "\n",
    "\n",
    "# Load stopwords, positive words, and negative words\n",
    "custom_stop_words = load_stopwords(stopwords_files, encoding='latin-1')\n",
    "positive_words, negative_words = load_pos_neg_words(positive_words_file, negative_words_file, encoding='latin-1')\n",
    "\n",
    "# Create a dictionary of positive and negative words\n",
    "pos_neg_dict = create_pos_neg_dict(positive_words, negative_words)\n",
    "# Create a dictionary of positive and negative words\n",
    "pos_neg_dict = create_pos_neg_dict(positive_words, negative_words)\n",
    "\n",
    "# Create a DataFrame to store the derived variables\n",
    "output_df = pd.DataFrame(columns=[\n",
    "    'URL_ID',\n",
    "    'URL',\n",
    "    'POSITIVE SCORE',\n",
    "    'NEGATIVE SCORE',\n",
    "    'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH',\n",
    "    'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT',\n",
    "    'SYLLABLE PER WORD',\n",
    "    'PERSONAL PRONOUNS',\n",
    "    'AVG WORD LENGTH'\n",
    "])\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    filename = f'{url_id}.txt'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        # Read the content from the file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "        # Clean text using custom stop words\n",
    "        cleaned_text = clean_text(text_content, custom_stop_words)\n",
    "\n",
    "        # Analyze text and extract derived variables\n",
    "        derived_variables = extract_derived_variables(cleaned_text)\n",
    "\n",
    "        # Append the results to the output DataFrame\n",
    "        output_df = output_df.append({\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'POSITIVE SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'NEGATIVE SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'POLARITY SCORE': derived_variables['Sentiment Scores'][0],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'SUBJECTIVITY SCORE': derived_variables['Sentiment Scores'][1],  # Assuming Sentiment Scores is a tuple (polarity, subjectivity)\n",
    "            'AVG SENTENCE LENGTH': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': (derived_variables['Complex Word Count'] / derived_variables['Word Count']) * 100,\n",
    "            'FOG INDEX': derived_variables['FOG INDEX'],  # You need to implement this in the extract_derived_variables function\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': derived_variables['Average Number of Words Per Sentence'],\n",
    "            'COMPLEX WORD COUNT': derived_variables['Complex Word Count'],\n",
    "            'WORD COUNT': derived_variables['Word Count'],\n",
    "            'SYLLABLE PER WORD': derived_variables['Syllable Count Per Word'],\n",
    "            'PERSONAL PRONOUNS': derived_variables['Personal Pronouns'],\n",
    "            'AVG WORD LENGTH': derived_variables['Average Word Length']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        print(f'Textual analysis for {url_id} completed successfully.')\n",
    "    else:\n",
    "        print(f'File {filename} not found for {url_id}.')\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_excel_file = 'Output.xlsx'\n",
    "output_df.to_excel(output_excel_file, index=False)\n",
    "print(f'Results saved to {output_excel_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00ade1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71667837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
